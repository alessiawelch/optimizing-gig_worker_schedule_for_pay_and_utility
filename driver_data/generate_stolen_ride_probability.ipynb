{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "524290e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WindowsPath('C:/Users/aless/OneDrive - Nexus365/Thesis/driver_data/combined_path/new_test/original/trips_with_price_duration_original_0075_v2_km_osrm.parquet') …\n",
      "Aggregating median price + ride count …\n",
      "3,721 origin–dest–hour buckets produced.\n",
      "168 origin-only buckets.\n",
      "\n",
      "   origin_super_row  origin_super_col time_group  median_price_usd  ride_count\n",
      "0                 0                 2  afternoon              6.85         878\n",
      "1                 1                 1  afternoon              7.25        1420\n",
      "2                 1                 1    evening              6.92        2522\n",
      "3                 0                 2    evening              6.29        1270\n",
      "4                 2                 1    evening              7.18        4232\n",
      "Combined table has 168 rows (3721 detailed  +  168 origin-only).\n",
      "Saved C:\\Users\\aless\\OneDrive - Nexus365\\Thesis\\driver_data\\combined_path\\new_test\\original\\origin_dest_hour_lookup_price_original_0075_v2.parquet  and  C:\\Users\\aless\\OneDrive - Nexus365\\Thesis\\driver_data\\combined_path\\new_test\\original\\origin_dest_hour_lookup_price_original_0075_v2.csv and C:\\Users\\aless\\OneDrive - Nexus365\\Thesis\\driver_data\\combined_path\\new_test\\original\\origin_dest_hour_lookup_price_original_0075_v2.npy\n",
      "Coverage & descriptive statistics\n",
      "universe size          : 4,704\n",
      "populated buckets      : 3,889  (82.7 %)\n",
      "missing buckets        : 815\n",
      "ride_count statistics\n",
      "min / 25% / 50% / 75% / max : 1 / 3 / 9 / 41 / 7698\n",
      "mean +- std : 75.85 +- 307.04\n",
      "median_price_usd statistics\n",
      "min / 25% / 50% / 75% / max : 3.75 / 8.35 / 12.74 / 17.15 / 61.89\n",
      "mean +- std 13.71 +- 6.78\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve().parents[0]     \n",
    "COMBINED_DIR   = PROJECT_ROOT / \"combined_path\"\n",
    "OG_DIR   = COMBINED_DIR / \"new_test\" / \"original\"\n",
    "CELL_FILE_ADDITION = \"original_0075_v2\"\n",
    "\n",
    "SRC_PARQUET      = OG_DIR / f\"trips_with_price_duration_{CELL_FILE_ADDITION}_km_osrm.parquet\"         \n",
    "DEST_FACTOR      = 6                        \n",
    "OUT_BASENAME     = OG_DIR / f\"origin_dest_hour_lookup_price_{CELL_FILE_ADDITION}\" \n",
    "\n",
    "\n",
    "\n",
    "def hour_to_group(h: int) -> str:\n",
    "    if   6 <= h < 10:       return \"early_morning\"\n",
    "    elif 10 <= h < 15:      return \"mid_day\"\n",
    "    elif 15 <= h < 18:      return \"afternoon\"\n",
    "    elif 18 <= h < 22:      return \"evening\"\n",
    "    elif h >= 22 or h < 2:  return \"night\"\n",
    "    else:                   return \"late_night\"          \n",
    "\n",
    "TG_LABELS = [\n",
    "    \"early_morning\", \"mid_day\", \"afternoon\",\n",
    "    \"evening\", \"night\", \"late_night\"\n",
    "]\n",
    "\n",
    "print(f\"Loading {SRC_PARQUET!r} …\")\n",
    "df = pd.read_parquet(SRC_PARQUET)\n",
    "\n",
    "req = {\"origin_row\",\"origin_col\",\"dest_row\",\"dest_col\",\"hour\",\"pay_after_uber_cut\"}\n",
    "missing = req.difference(df.columns.str.lower())\n",
    "if missing:\n",
    "    raise ValueError(f\"input file is missing columns: {sorted(missing)}\")\n",
    "\n",
    "\n",
    "df[\"time_group\"]      = df[\"hour\"].astype(int).apply(hour_to_group)\n",
    "df[\"origin_super_row\"] = (df[\"origin_row\"] // DEST_FACTOR).astype(\"int16\")\n",
    "df[\"origin_super_col\"] = (df[\"origin_col\"] // DEST_FACTOR).astype(\"int16\")\n",
    "df[\"dest_super_row\"] = (df[\"dest_row\"] // DEST_FACTOR).astype(\"int16\")\n",
    "df[\"dest_super_col\"] = (df[\"dest_col\"] // DEST_FACTOR).astype(\"int16\")\n",
    "\n",
    "MAX_COLS = 7\n",
    "df[\"origin_super_col\"] = (df[\"origin_col\"] // DEST_FACTOR).clip(upper=MAX_COLS-1).astype(\"int16\")\n",
    "df[\"dest_super_col\"]   = (df[\"dest_col\"]   // DEST_FACTOR).clip(upper=MAX_COLS-1).astype(\"int16\")\n",
    "\n",
    "print(\"Aggregating median price + ride count …\")\n",
    "agg = (\n",
    "    df.groupby(\n",
    "        [\"origin_super_row\",\"origin_super_col\",\n",
    "         \"dest_super_row\",\"dest_super_col\",\n",
    "         \"time_group\"],\n",
    "        sort=False, observed=True\n",
    "    )[\"pay_after_uber_cut\"]\n",
    "      .agg(median_price_usd=\"median\", ride_count=\"size\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "print(f\"{len(agg):,} origin–dest–hour buckets produced.\")\n",
    "\n",
    "agg_origin = (\n",
    "    df.groupby(\n",
    "        [\"origin_super_row\",\"origin_super_col\",\"time_group\"],\n",
    "        sort=False, observed=True\n",
    "    )[\"pay_after_uber_cut\"]\n",
    "      .agg(median_price_usd=\"median\", ride_count=\"size\")\n",
    "      .reset_index()\n",
    ")\n",
    "print(f\"{len(agg_origin):,} origin-only buckets.\\n\")\n",
    "\n",
    "print(agg_origin.head())\n",
    "\n",
    "\n",
    "agg_origin_app = agg_origin.copy()\n",
    "agg_origin_app[\"dest_super_row\"] = pd.NA\n",
    "agg_origin_app[\"dest_super_col\"] = pd.NA\n",
    "\n",
    "COLS = [\n",
    "    \"origin_super_row\", \"origin_super_col\",\n",
    "    \"dest_super_row\",   \"dest_super_col\",\n",
    "    \"time_group\",\n",
    "    \"median_price_usd\", \"ride_count\"\n",
    "]\n",
    "agg_origin_app = agg_origin_app[COLS]\n",
    "agg            = agg[COLS]          \n",
    "\n",
    "\n",
    "agg_combined = pd.concat([agg, agg_origin_app], ignore_index=True, sort=False)\n",
    "\n",
    "print(f\"Combined table has {len(agg_origin):,} rows \"\n",
    "      f\"({len(agg)} detailed  +  {len(agg_origin_app)} origin-only).\")\n",
    "\n",
    "\n",
    "base = Path(OUT_BASENAME)\n",
    "OUT_PARQUET = base.with_suffix(\".parquet\")\n",
    "OUT_CSV = base.with_suffix(\".csv\")\n",
    "OUT_NUMPY = base.with_suffix(\".npy\")\n",
    "\n",
    "agg_origin.to_parquet(OUT_PARQUET, compression=\"zstd\")\n",
    "agg_combined.to_csv(OUT_CSV, index=False)\n",
    "np.save(OUT_NUMPY, agg_combined.to_numpy())\n",
    "\n",
    "print(f\"Saved {base}.parquet  and  {base}.csv and {base}.npy\")\n",
    "\n",
    "print(\"Coverage & descriptive statistics\")\n",
    "\n",
    "n_orow = df[\"origin_super_row\"].max() + 1\n",
    "n_ocol = df[\"origin_super_col\"].max() + 1\n",
    "n_drow = df[\"dest_super_row\"].max() + 1\n",
    "n_dcol = df[\"dest_super_col\"].max() + 1\n",
    "TOTAL  = n_orow * n_ocol * n_drow * n_dcol * len(TG_LABELS)\n",
    "\n",
    "missing_cnt = TOTAL - len(agg_combined)\n",
    "cov_pct     = 100 * len(agg_combined) / TOTAL\n",
    "\n",
    "print(f\"universe size          : {TOTAL:,}\")\n",
    "print(f\"populated buckets      : {len(agg_combined):,}  ({cov_pct:4.1f} %)\")\n",
    "print(f\"missing buckets        : {missing_cnt:,}\")\n",
    "\n",
    "rc = agg_combined[\"ride_count\"]\n",
    "print(\"ride_count statistics\")\n",
    "print(f\"min / 25% / 50% / 75% / max : \"\n",
    "      f\"{rc.min():.0f} / {rc.quantile(.25):.0f} / {rc.median():.0f} / \"\n",
    "      f\"{rc.quantile(.75):.0f} / {rc.max():.0f}\")\n",
    "print(f\"mean +- std : {rc.mean():.2f} +- {rc.std():.2f}\")\n",
    "\n",
    "mp = agg_combined[\"median_price_usd\"]\n",
    "print(\"median_price_usd statistics\")\n",
    "print(f\"min / 25% / 50% / 75% / max : \"\n",
    "      f\"{mp.min():.2f} / {mp.quantile(.25):.2f} / {mp.median():.2f} / \"\n",
    "      f\"{mp.quantile(.75):.2f} / {mp.max():.2f}\")\n",
    "print(f\"mean +- std {mp.mean():.2f} +- {mp.std():.2f}\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85b5dc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WindowsPath('C:/Users/aless/OneDrive - Nexus365/Thesis/driver_data/combined_path/new_test/original/trips_with_price_duration_original_0075_v2_km_osrm.parquet') …\n",
      "Aggregating median distance + ride count …\n",
      "3,721 origin–dest–hour buckets produced.\n",
      "      → 168 origin-only buckets.\n",
      "\n",
      "Combined table has 3,889 rows (3721 detailed + 168 origin-only).\n",
      "Saved lookup tables to C:\\Users\\aless\\OneDrive - Nexus365\\Thesis\\driver_data\\combined_path\\new_test\\original\\origin_dest_hour_lookup_distance_original_0075_v2.[parquet|csv|npy]\n",
      "Coverage & descriptive statistics\n",
      "universe size     : 4,704\n",
      "populated buckets : 3,889  (82.7 %)\n",
      "\n",
      "   ride_count statistics\n",
      "count    3889.00\n",
      "mean       75.85\n",
      "std       307.04\n",
      "min         1.00\n",
      "25%         3.00\n",
      "50%         9.00\n",
      "75%        41.00\n",
      "max      7698.00\n",
      "Name: ride_count, dtype: float64\n",
      "\n",
      "   median_dist_km statistics\n",
      "count    3889.00\n",
      "mean       11.43\n",
      "std         6.41\n",
      "min         1.16\n",
      "25%         6.08\n",
      "50%        10.83\n",
      "75%        15.34\n",
      "max        62.14\n",
      "Name: median_dist_km, dtype: float64\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "SRC_PARQUET      = OG_DIR / f\"trips_with_price_duration_{CELL_FILE_ADDITION}_km_osrm.parquet\"         \n",
    "DEST_FACTOR      = 6                        \n",
    "OUT_BASENAME     = OG_DIR / f\"origin_dest_hour_lookup_distance_{CELL_FILE_ADDITION}\" \n",
    "\n",
    "print(f\"Loading {SRC_PARQUET!r} …\")\n",
    "df = pd.read_parquet(SRC_PARQUET)\n",
    "\n",
    "req = {\n",
    "    \"origin_row\",\"origin_col\",\"dest_row\",\"dest_col\",\n",
    "    \"hour\",\"trip_distance_km\"           \n",
    "}\n",
    "missing = req.difference(df.columns.str.lower())\n",
    "if missing:\n",
    "    raise ValueError(f\"input file is missing columns: {sorted(missing)}\")\n",
    "\n",
    "\n",
    "df[\"time_group\"]       = df[\"hour\"].astype(int).apply(hour_to_group)\n",
    "df[\"origin_super_row\"] = (df[\"origin_row\"] // DEST_FACTOR).astype(\"int16\")\n",
    "df[\"origin_super_col\"] = (df[\"origin_col\"] // DEST_FACTOR).astype(\"int16\")\n",
    "df[\"dest_super_row\"]   = (df[\"dest_row\"]   // DEST_FACTOR).astype(\"int16\")\n",
    "df[\"dest_super_col\"]   = (df[\"dest_col\"]   // DEST_FACTOR).astype(\"int16\")\n",
    "\n",
    "MAX_COLS = 7\n",
    "df[\"origin_super_col\"] = (df[\"origin_col\"] // DEST_FACTOR).clip(upper=MAX_COLS-1).astype(\"int16\")\n",
    "df[\"dest_super_col\"]   = (df[\"dest_col\"]   // DEST_FACTOR).clip(upper=MAX_COLS-1).astype(\"int16\")\n",
    "\n",
    "\n",
    "print(\"Aggregating median distance + ride count …\")\n",
    "agg = (\n",
    "    df.groupby(\n",
    "        [\"origin_super_row\",\"origin_super_col\",\n",
    "         \"dest_super_row\",\"dest_super_col\",\n",
    "         \"time_group\"],\n",
    "        sort=False, observed=True\n",
    "    )[\"trip_distance_km\"]               \n",
    "      .agg(median_dist_km=\"median\", ride_count=\"size\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "print(f\"{len(agg):,} origin–dest–hour buckets produced.\")\n",
    "\n",
    "\n",
    "agg_origin = (\n",
    "    df.groupby(\n",
    "        [\"origin_super_row\",\"origin_super_col\",\"time_group\"],\n",
    "        sort=False, observed=True\n",
    "    )[\"trip_distance_km\"]                 \n",
    "      .agg(median_dist_km=\"median\", ride_count=\"size\")\n",
    "      .reset_index()\n",
    ")\n",
    "print(f\"      → {len(agg_origin):,} origin-only buckets.\\n\")\n",
    "\n",
    "\n",
    "TOTAL_RIDES = len(df)\n",
    "agg[\"rf_global\"] = agg[\"ride_count\"] / TOTAL_RIDES    \n",
    "\n",
    "\n",
    "agg_origin[\"rf_global\"] = agg_origin[\"ride_count\"] / TOTAL_RIDES\n",
    "\n",
    "\n",
    "agg_origin_app = agg_origin.copy()\n",
    "agg_origin_app[\"dest_super_row\"] = pd.NA\n",
    "agg_origin_app[\"dest_super_col\"] = pd.NA\n",
    "\n",
    "\n",
    "COLS = [\n",
    "    \"origin_super_row\",\"origin_super_col\",\n",
    "    \"dest_super_row\",\"dest_super_col\",\n",
    "    \"time_group\",\n",
    "    \"median_dist_km\",\"ride_count\",\"rf_global\"            \n",
    "]\n",
    "agg_origin_app = agg_origin_app[COLS]\n",
    "agg            = agg.reindex(columns=COLS, fill_value=pd.NA)\n",
    "agg_combined   = pd.concat([agg, agg_origin_app], ignore_index=True, sort=False)\n",
    "\n",
    "print(f\"Combined table has {len(agg_combined):,} rows \"\n",
    "      f\"({len(agg)} detailed + {len(agg_origin_app)} origin-only).\")\n",
    "\n",
    "\n",
    "base = Path(OUT_BASENAME)\n",
    "agg_combined.to_parquet(base.with_suffix(\".parquet\"), compression=\"zstd\")\n",
    "agg_combined.to_csv    (base.with_suffix(\".csv\"),     index=False)\n",
    "np.save                (base.with_suffix(\".npy\"),     agg_combined.to_numpy())\n",
    "print(f\"Saved lookup tables to {base}.[parquet|csv|npy]\")\n",
    "\n",
    "\n",
    "print(\"Coverage & descriptive statistics\")\n",
    "n_orow = df[\"origin_super_row\"].max() + 1\n",
    "n_ocol = df[\"origin_super_col\"].max() + 1\n",
    "n_drow = df[\"dest_super_row\"].max() + 1\n",
    "n_dcol = df[\"dest_super_col\"].max() + 1\n",
    "TOTAL  = n_orow * n_ocol * n_drow * n_dcol * len(TG_LABELS)\n",
    "\n",
    "cov_pct = 100 * len(agg_combined) / TOTAL\n",
    "print(f\"universe size     : {TOTAL:,}\")\n",
    "print(f\"populated buckets : {len(agg_combined):,}  ({cov_pct:4.1f} %)\")\n",
    "\n",
    "print(\"\\n   ride_count statistics\")\n",
    "print(agg_combined[\"ride_count\"].describe(percentiles=[.25,.5,.75]).round(2))\n",
    "\n",
    "print(\"\\n   median_dist_km statistics\")\n",
    "print(agg_combined[\"median_dist_km\"].describe(percentiles=[.25,.5,.75]).round(2))\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f23d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WindowsPath('C:/Users/aless/OneDrive - Nexus365/Thesis/driver_data/combined_path/new_test/original/trips_with_price_duration_original_0075_v2_km_osrm.parquet') …\n",
      "      147,498 rides across 2821 calendar day(s)\n",
      "Aggregating origin-only buckets …\n",
      "168 origin-only rows.\n",
      "Saved to C:\\Users\\aless\\OneDrive - Nexus365\\Thesis\\driver_data\\combined_path\\new_test\\original\\origin_dest_hour_lookup_demand_original_0075_v2.[parquet|csv|npy]\n",
      "Coverage & descriptive stats\n",
      "universe size     : 4,704\n",
      "populated buckets : 168  ( 3.6 %)\n",
      "ride_count statistics\n",
      "count     168.00\n",
      "mean      877.96\n",
      "std      1121.87\n",
      "min        12.00\n",
      "25%       252.00\n",
      "50%       556.00\n",
      "75%       945.50\n",
      "max      7698.00\n",
      "Name: ride_count, dtype: float64\n",
      "rides_per_day statistics\n",
      "count    168.00\n",
      "mean       0.31\n",
      "std        0.40\n",
      "min        0.00\n",
      "25%        0.09\n",
      "50%        0.20\n",
      "75%        0.34\n",
      "max        2.73\n",
      "Name: rides_per_day, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "SRC_PARQUET      = OG_DIR / f\"trips_with_price_duration_{CELL_FILE_ADDITION}_km_osrm.parquet\"         \n",
    "DEST_FACTOR      = 6                        \n",
    "OUT_BASENAME     = OG_DIR / f\"origin_dest_hour_lookup_demand_{CELL_FILE_ADDITION}\"           \n",
    "\n",
    "print(f\"Loading {SRC_PARQUET!r} …\")\n",
    "df = pd.read_parquet(SRC_PARQUET)\n",
    "\n",
    "REQUIRED = {\"origin_row\", \"origin_col\",\n",
    "            \"dest_row\",   \"dest_col\",\n",
    "            \"hour\",       \"begintrip_timestamp_london\"}\n",
    "missing = REQUIRED.difference(df.columns.str.lower())\n",
    "if missing:\n",
    "    raise ValueError(f\"Input file is missing columns: {sorted(missing)}\")\n",
    "\n",
    "\n",
    "df[\"pickup_date\"] = (\n",
    "    pd.to_datetime(df[\"begintrip_timestamp_london\"])\n",
    "      .dt.normalize()\n",
    ")\n",
    "\n",
    "N_DAYS_TOTAL = df[\"pickup_date\"].nunique()\n",
    "TOTAL_RIDES  = len(df)\n",
    "print(f\"      {TOTAL_RIDES:,} rides across {N_DAYS_TOTAL} calendar day(s)\")\n",
    "\n",
    "\n",
    "df[\"time_group\"]       = df[\"hour\"].astype(int).apply(hour_to_group)\n",
    "df[\"origin_super_row\"] = (df[\"origin_row\"] // DEST_FACTOR).astype(\"int16\")\n",
    "df[\"origin_super_col\"] = (df[\"origin_col\"] // DEST_FACTOR).astype(\"int16\")\n",
    "df[\"dest_super_row\"]   = (df[\"dest_row\"]   // DEST_FACTOR).astype(\"int16\")\n",
    "df[\"dest_super_col\"]   = (df[\"dest_col\"]   // DEST_FACTOR).astype(\"int16\")\n",
    "\n",
    "MAX_COLS = 7\n",
    "df[\"origin_super_col\"] = (df[\"origin_col\"] // DEST_FACTOR).clip(upper=MAX_COLS-1).astype(\"int16\")\n",
    "df[\"dest_super_col\"]   = (df[\"dest_col\"]   // DEST_FACTOR).clip(upper=MAX_COLS-1).astype(\"int16\")\n",
    "\n",
    "print(\"Aggregating origin-only buckets …\")\n",
    "agg_origin = (\n",
    "    df.groupby(\n",
    "        [\"origin_super_row\",\"origin_super_col\",\"time_group\"],\n",
    "        sort=False, observed=True\n",
    "    )\n",
    "    .size()\n",
    "    .reset_index(name=\"ride_count\")\n",
    ")\n",
    "\n",
    "agg_origin[\"rides_per_day\"] = agg_origin[\"ride_count\"] / N_DAYS_TOTAL\n",
    "agg_origin[\"rf_global\"]     = agg_origin[\"ride_count\"] / TOTAL_RIDES\n",
    "agg_origin[\"rf_time_group\"] = (\n",
    "    agg_origin.groupby(\"time_group\", observed=True)[\"ride_count\"]\n",
    "              .transform(lambda s: s / s.sum())\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"{len(agg_origin):,} origin-only rows.\")\n",
    "\n",
    "base = Path(OUT_BASENAME)\n",
    "agg_origin.to_parquet(base.with_suffix(\".parquet\"), compression=\"zstd\")\n",
    "agg_origin.to_csv    (base.with_suffix(\".csv\"),     index=False)\n",
    "np.save              (base.with_suffix(\".npy\"),     agg_origin.to_numpy())\n",
    "\n",
    "print(f\"Saved to {base}.[parquet|csv|npy]\")\n",
    "\n",
    "\n",
    "print(\"Coverage & descriptive stats\")\n",
    "\n",
    "n_or = df[\"origin_super_row\"].max() + 1\n",
    "n_oc = df[\"origin_super_col\"].max() + 1\n",
    "n_dr = df[\"dest_super_row\"].max()   + 1\n",
    "n_dc = df[\"dest_super_col\"].max()   + 1\n",
    "TOTAL_UNIVERSE = n_or * n_oc * n_dr * n_dc * len(TG_LABELS)\n",
    "\n",
    "print(f\"universe size     : {TOTAL_UNIVERSE:,}\")\n",
    "print(f\"populated buckets : {len(agg_origin):,}  \"\n",
    "      f\"({100*len(agg_origin)/TOTAL_UNIVERSE:4.1f} %)\")\n",
    "\n",
    "print(\"ride_count statistics\")\n",
    "print(agg_origin[\"ride_count\"].describe(percentiles=[.25,.5,.75]).round(2))\n",
    "\n",
    "print(\"rides_per_day statistics\")\n",
    "print(agg_origin[\"rides_per_day\"].describe(percentiles=[.25,.5,.75]).round(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4b061df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WindowsPath('C:/Users/aless/OneDrive - Nexus365/Thesis/driver_data/combined_path/new_test/original/trips_with_price_duration_original_0075_v2_km_osrm.parquet') …\n",
      "      147,498 rides across 2821 calendar day(s)\n",
      "Aggregating origin-only buckets …\n",
      "      24 origin-only rows.\n",
      "Saved to C:\\Users\\aless\\OneDrive - Nexus365\\Thesis\\driver_data\\combined_path\\new_test\\original\\hour_lookup_demand_original_0075_v2.[parquet|csv|npy]\n"
     ]
    }
   ],
   "source": [
    "SRC_PARQUET      = OG_DIR / f\"trips_with_price_duration_{CELL_FILE_ADDITION}_km_osrm.parquet\"         \n",
    "DEST_FACTOR      = 6                        \n",
    "OUT_BASENAME     = OG_DIR / f\"hour_lookup_demand_{CELL_FILE_ADDITION}\"     \n",
    "\n",
    "\n",
    "print(f\"Loading {SRC_PARQUET!r} …\")\n",
    "df = pd.read_parquet(SRC_PARQUET)\n",
    "\n",
    "REQUIRED = {\"hour\", \"begintrip_timestamp_london\"}\n",
    "missing = REQUIRED.difference(df.columns.str.lower())\n",
    "if missing:\n",
    "    raise ValueError(f\"Input file is missing columns: {sorted(missing)}\")\n",
    "\n",
    "\n",
    "df[\"pickup_date\"] = (\n",
    "    pd.to_datetime(df[\"begintrip_timestamp_london\"])\n",
    "      .dt.normalize()\n",
    ")\n",
    "\n",
    "N_DAYS_TOTAL = df[\"pickup_date\"].nunique()\n",
    "TOTAL_RIDES  = len(df)\n",
    "print(f\"      {TOTAL_RIDES:,} rides across {N_DAYS_TOTAL} calendar day(s)\")\n",
    "\n",
    "\n",
    "df[\"time_group\"]       = df[\"hour\"].astype(int).apply(hour_to_group)\n",
    "\n",
    "\n",
    "print(\"Aggregating origin-only buckets …\")\n",
    "agg_origin = (\n",
    "    df.groupby(\n",
    "        [\"hour\"],\n",
    "        sort=False, observed=True\n",
    "    )\n",
    "    .size()\n",
    "    .reset_index(name=\"ride_count\")\n",
    ")\n",
    "\n",
    "agg_origin[\"rides_per_day\"] = agg_origin[\"ride_count\"] / N_DAYS_TOTAL\n",
    "agg_origin[\"rf_global\"]     = agg_origin[\"ride_count\"] / TOTAL_RIDES\n",
    "agg_origin[\"rf_global_adjusted\"] = agg_origin[\"rf_global\"] * 10\n",
    "\n",
    "print(f\"      {len(agg_origin):,} origin-only rows.\")\n",
    "\n",
    "\n",
    "base = Path(OUT_BASENAME)\n",
    "agg_origin.to_parquet(base.with_suffix(\".parquet\"), compression=\"zstd\")\n",
    "agg_origin.to_csv    (base.with_suffix(\".csv\"),     index=False)\n",
    "np.save              (base.with_suffix(\".npy\"),     agg_origin.to_numpy())\n",
    "\n",
    "print(f\"Saved to {base}.[parquet|csv|npy]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa537372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WindowsPath('C:/Users/aless/OneDrive - Nexus365/Thesis/driver_data/combined_path/new_test/original/trips_original_0075_v2_with_predicted_information.parquet') …\n",
      "   origin_row  origin_col  dest_row  dest_col  begin_lat  begin_lng  \\\n",
      "0           5          12         7         7  51.440338  -0.159358   \n",
      "1           6           7         9        11  51.445763  -0.191400   \n",
      "2          10          11        12         8  51.479115  -0.166910   \n",
      "3          11           7        13         6  51.487488  -0.191229   \n",
      "4           5          13        13         8  51.444721  -0.148535   \n",
      "\n",
      "     end_lat   end_lng  haversine_km begintrip_timestamp_london  ...  \\\n",
      "0  51.456711 -0.191571      2.880576  2016-04-28 17:23:20+01:00  ...   \n",
      "1  51.474430 -0.167369      3.596283  2016-04-28 17:50:48+01:00  ...   \n",
      "2  51.490761 -0.183790      1.744453  2016-04-28 18:10:50+01:00  ...   \n",
      "3  51.502617 -0.199705      1.781674  2016-04-28 18:24:59+01:00  ...   \n",
      "4  51.502235 -0.186893      6.925133  2016-04-28 21:06:02+01:00  ...   \n",
      "\n",
      "  trip_distance_miles  trip_distance_km     osrm_sec  osrm_km dow  month_idx  \\\n",
      "0            2.789894          4.489888   398.700012   3.4857   3      24196   \n",
      "1            3.236049          5.207903   626.099976   5.0948   3      24196   \n",
      "2            1.491487          2.400309   302.100006   1.9474   3      24196   \n",
      "3            1.642280          2.642988   389.100006   2.6575   3      24196   \n",
      "4            8.204270         13.203459  1175.900024   7.8614   3      24196   \n",
      "\n",
      "   doy   km_pred     sec_pred  price_pred  \n",
      "0  119  3.625828   664.820351    5.677524  \n",
      "1  119  5.318336   955.559975    7.482545  \n",
      "2  119  2.202951   542.938647    5.192629  \n",
      "3  119  2.887826   690.397036    5.808538  \n",
      "4  119  8.588139  1328.159043    9.898572  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "Aggregating median rate and ride count\n",
      "3,721 origin–dest–hour buckets produced\n",
      "168 origin-only buckets.\n",
      "\n",
      "   origin_super_row  origin_super_col time_group  median_rate_usd  ride_count\n",
      "0                 0                 2  afternoon         0.501459         878\n",
      "1                 1                 1  afternoon         0.497190        1420\n",
      "2                 1                 1    evening         0.542662        2522\n",
      "3                 0                 2    evening         0.551687        1270\n",
      "4                 2                 1    evening         0.516680        4232\n",
      "[3/5] Combined table has 3,889 rows (3721 detailed  +  168 origin-only).\n",
      "Saved C:\\Users\\aless\\OneDrive - Nexus365\\Thesis\\driver_data\\combined_path\\new_test\\original\\origin_dest_hour_lookup_rate_original_0075_v2.parquet / .csv / .npy\n",
      "Coverage & descriptive statistics\n",
      "universe size       : 4,704\n",
      "populated buckets   : 3,889  (82.7 %)\n",
      "missing buckets     : 815\n",
      "ride_count statistics\n",
      "      min / 25% / 50% / 75% / max : 1 / 3 / 9 / 41 / 7698\n",
      "      mean +- std                 : 75.85 +- 307.04\n",
      "median_rate_usd statistics\n",
      "      min / 25% / 50% / 75% / max : 0.10 / 0.42 / 0.49 / 0.57 / 1.80\n",
      "      mean +- std                 : 0.51 +- 0.12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SRC_PARQUET      = OG_DIR / f\"trips_{CELL_FILE_ADDITION}_with_predicted_information.parquet\"         \n",
    "DEST_FACTOR      = 6                        \n",
    "OUT_BASENAME     = OG_DIR / f\"origin_dest_hour_lookup_rate_{CELL_FILE_ADDITION}\"     \n",
    "\n",
    "\n",
    "\n",
    "print(f\"Loading {SRC_PARQUET!r} …\")\n",
    "df = pd.read_parquet(SRC_PARQUET)\n",
    "print(df.head())\n",
    "\n",
    "req_cols = {\n",
    "    \"origin_row\", \"origin_col\", \"dest_row\", \"dest_col\",\n",
    "    \"hour\", \"pay_after_uber_cut\",\n",
    "    \"sec_pred\"\n",
    "}\n",
    "present = {c.lower() for c in df.columns}\n",
    "if not ((\"sec_pred\" in present)):\n",
    "    raise ValueError(\"input file needs either 'trip_duration_min' or 'trip_duration_sec'\")\n",
    "missing = {c for c in req_cols if c not in present}\n",
    "if missing:\n",
    "    raise ValueError(f\"input file is missing columns: {sorted(missing)}\")\n",
    "\n",
    "\n",
    "df[\"duration_min\"] = df[\"sec_pred\"] / 60.0\n",
    "\n",
    "df[\"rate_usd_per_min\"] = df[\"pay_after_uber_cut\"] / df[\"duration_min\"].clip(lower=0.5)\n",
    "\n",
    "\n",
    "df[\"time_group\"]       = df[\"hour\"].astype(int).apply(hour_to_group)\n",
    "df[\"origin_super_row\"] = (df[\"origin_row\"] // DEST_FACTOR).astype(\"int16\")\n",
    "df[\"origin_super_col\"] = (df[\"origin_col\"] // DEST_FACTOR).astype(\"int16\")\n",
    "df[\"dest_super_row\"]   = (df[\"dest_row\"]   // DEST_FACTOR).astype(\"int16\")\n",
    "df[\"dest_super_col\"]   = (df[\"dest_col\"]   // DEST_FACTOR).astype(\"int16\")\n",
    "MAX_COLS = 7\n",
    "df[\"origin_super_col\"] = (df[\"origin_col\"] // DEST_FACTOR).clip(upper=MAX_COLS-1).astype(\"int16\")\n",
    "df[\"dest_super_col\"]   = (df[\"dest_col\"]   // DEST_FACTOR).clip(upper=MAX_COLS-1).astype(\"int16\")\n",
    "\n",
    "\n",
    "print(\"Aggregating median rate and ride count\")\n",
    "agg = (\n",
    "    df.groupby(\n",
    "        [\"origin_super_row\",\"origin_super_col\",\n",
    "         \"dest_super_row\",\"dest_super_col\",\n",
    "         \"time_group\"],\n",
    "        sort=False, observed=True\n",
    "    )[\"rate_usd_per_min\"]\n",
    "      .agg(median_rate_usd=\"median\", ride_count=\"size\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "print(f\"{len(agg):,} origin–dest–hour buckets produced\")\n",
    "\n",
    "agg_origin = (\n",
    "    df.groupby(\n",
    "        [\"origin_super_row\",\"origin_super_col\",\"time_group\"],\n",
    "        sort=False, observed=True\n",
    "    )[\"rate_usd_per_min\"]\n",
    "      .agg(median_rate_usd=\"median\", ride_count=\"size\")\n",
    "      .reset_index()\n",
    ")\n",
    "print(f\"{len(agg_origin):,} origin-only buckets.\\n\")\n",
    "print(agg_origin.head())\n",
    "\n",
    "\n",
    "agg_origin_app = agg_origin.copy()\n",
    "agg_origin_app[\"dest_super_row\"] = pd.NA\n",
    "agg_origin_app[\"dest_super_col\"] = pd.NA\n",
    "\n",
    "COLS = [\n",
    "    \"origin_super_row\",\"origin_super_col\",\n",
    "    \"dest_super_row\",\"dest_super_col\",\n",
    "    \"time_group\",\n",
    "    \"median_rate_usd\",\"ride_count\"\n",
    "]\n",
    "agg_combined = pd.concat(\n",
    "    [agg[COLS], agg_origin_app[COLS]],\n",
    "    ignore_index=True, sort=False\n",
    ")\n",
    "\n",
    "print(f\"[3/5] Combined table has {len(agg_combined):,} rows \"\n",
    "      f\"({len(agg)} detailed  +  {len(agg_origin_app)} origin-only).\")\n",
    "\n",
    "\n",
    "base = Path(OUT_BASENAME)\n",
    "agg_combined.to_parquet(base.with_suffix(\".parquet\"), compression=\"zstd\")\n",
    "agg_combined.to_csv(base.with_suffix(\".csv\"), index=False)\n",
    "np.save(base.with_suffix(\".npy\"), agg_combined.to_numpy())\n",
    "\n",
    "print(f\"Saved {base}.parquet / .csv / .npy\")\n",
    "\n",
    "\n",
    "print(\"Coverage & descriptive statistics\")\n",
    "\n",
    "n_orow = df[\"origin_super_row\"].max() + 1\n",
    "n_ocol = df[\"origin_super_col\"].max() + 1\n",
    "n_drow = df[\"dest_super_row\"].max() + 1\n",
    "n_dcol = df[\"dest_super_col\"].max() + 1\n",
    "UNIVERSE = n_orow * n_ocol * n_drow * n_dcol * len(TG_LABELS)\n",
    "\n",
    "missing_cnt = UNIVERSE - len(agg_combined)\n",
    "cov_pct      = 100 * len(agg_combined) / UNIVERSE\n",
    "print(f\"universe size       : {UNIVERSE:,}\")\n",
    "print(f\"populated buckets   : {len(agg_combined):,}  ({cov_pct:4.1f} %)\")\n",
    "print(f\"missing buckets     : {missing_cnt:,}\")\n",
    "\n",
    "rc = agg_combined[\"ride_count\"]\n",
    "print(\"ride_count statistics\")\n",
    "print(f\"      min / 25% / 50% / 75% / max : \"\n",
    "      f\"{rc.min():.0f} / {rc.quantile(.25):.0f} / {rc.median():.0f} / \"\n",
    "      f\"{rc.quantile(.75):.0f} / {rc.max():.0f}\")\n",
    "print(f\"      mean +- std                 : {rc.mean():.2f} +- {rc.std():.2f}\")\n",
    "\n",
    "rp = agg_combined[\"median_rate_usd\"]\n",
    "print(\"median_rate_usd statistics\")\n",
    "print(f\"      min / 25% / 50% / 75% / max : \"\n",
    "      f\"{rp.min():.2f} / {rp.quantile(.25):.2f} / {rp.median():.2f} / \"\n",
    "      f\"{rp.quantile(.75):.2f} / {rp.max():.2f}\")\n",
    "print(f\"      mean +- std                 : {rp.mean():.2f} +- {rp.std():.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "driver_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
